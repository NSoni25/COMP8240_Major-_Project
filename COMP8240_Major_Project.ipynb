{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP8240_Major_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG5QztZWNumQ"
      },
      "source": [
        "# Abstractive Summaries of online articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca9C5UvANZ5H"
      },
      "source": [
        "Installing Pytorch, sentencepiece, rougescore and Hugging Face Transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSkHhMrUzz4p",
        "outputId": "df08d841-b6b0-4cc4-8509-a895555f5182"
      },
      "source": [
        "!pip3 install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
            "Requirement already satisfied: torch==1.8.2+cu111 in /usr/local/lib/python3.7/dist-packages (1.8.2+cu111)\n",
            "Requirement already satisfied: torchvision==0.9.2+cu111 in /usr/local/lib/python3.7/dist-packages (0.9.2+cu111)\n",
            "Requirement already satisfied: torchaudio===0.8.2 in /usr/local/lib/python3.7/dist-packages (0.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.2+cu111) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.2+cu111) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.2+cu111) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S2RTjE3RimX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6fcf283-b103-4ac1-bf3b-c8998a0f9b55"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT5hKDTeRmzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0af28baa-7296-48f6-df7c-f48df6d6b7e9"
      },
      "source": [
        "pip install rouge_score"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (0.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NsZ3lpMMjrB",
        "outputId": "f7ecf3eb-a030-4c8b-8132-c8bee39afddd"
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rz6D3QeOAol"
      },
      "source": [
        "Importing libraries for generating abstractive summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeHiYDfQ1mPC"
      },
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import torch\n",
        "from rouge_score import rouge_scorer"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPLsmeB-lzU2"
      },
      "source": [
        "#Model intialization\n",
        "model_name = \"google/pegasus-reddit_tifu\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emp-Ztq-zbiZ"
      },
      "source": [
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph2pjmOsykSx",
        "outputId": "47dd0552-c39c-4fa6-890c-d433fc8c4f7c"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreTrainedTokenizer(name_or_path='google/pegasus-reddit_tifu', vocab_size=96103, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask_2>', 'additional_special_tokens': ['<mask_1>', '<unk_2>', '<unk_3>', '<unk_4>', '<unk_5>', '<unk_6>', '<unk_7>', '<unk_8>', '<unk_9>', '<unk_10>', '<unk_11>', '<unk_12>', '<unk_13>', '<unk_14>', '<unk_15>', '<unk_16>', '<unk_17>', '<unk_18>', '<unk_19>', '<unk_20>', '<unk_21>', '<unk_22>', '<unk_23>', '<unk_24>', '<unk_25>', '<unk_26>', '<unk_27>', '<unk_28>', '<unk_29>', '<unk_30>', '<unk_31>', '<unk_32>', '<unk_33>', '<unk_34>', '<unk_35>', '<unk_36>', '<unk_37>', '<unk_38>', '<unk_39>', '<unk_40>', '<unk_41>', '<unk_42>', '<unk_43>', '<unk_44>', '<unk_45>', '<unk_46>', '<unk_47>', '<unk_48>', '<unk_49>', '<unk_50>', '<unk_51>', '<unk_52>', '<unk_53>', '<unk_54>', '<unk_55>', '<unk_56>', '<unk_57>', '<unk_58>', '<unk_59>', '<unk_60>', '<unk_61>', '<unk_62>', '<unk_63>', '<unk_64>', '<unk_65>', '<unk_66>', '<unk_67>', '<unk_68>', '<unk_69>', '<unk_70>', '<unk_71>', '<unk_72>', '<unk_73>', '<unk_74>', '<unk_75>', '<unk_76>', '<unk_77>', '<unk_78>', '<unk_79>', '<unk_80>', '<unk_81>', '<unk_82>', '<unk_83>', '<unk_84>', '<unk_85>', '<unk_86>', '<unk_87>', '<unk_88>', '<unk_89>', '<unk_90>', '<unk_91>', '<unk_92>', '<unk_93>', '<unk_94>', '<unk_95>', '<unk_96>', '<unk_97>', '<unk_98>', '<unk_99>', '<unk_100>', '<unk_101>', '<unk_102>']})"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cF9csIoLUfM"
      },
      "source": [
        "model_dic = []\n",
        "mod = 'Reddit_tifu'\n",
        "data_souce = 'articles'\n",
        "blog = [\"https://datascience.codata.org/articles/10.5334/dsj-2021-009/\",\n",
        "        \"https://datascience.codata.org/articles/10.5334/dsj-2021-027/\",\n",
        "       \"https://datascience.codata.org/articles/10.5334/dsj-2021-026/\",\n",
        "       \"https://datascience.codata.org/articles/10.5334/dsj-2021-025/\",\n",
        "       \"https://datascience.codata.org/articles/10.5334/dsj-2021-024/\",\n",
        "       \"https://datascience.codata.org/articles/10.5334/dsj-2020-015/\",\n",
        "       \"https://datascience.codata.org/articles/10.5334/dsj-2020-013/\",\n",
        "       \"https://datascience.codata.org/articles/10.5334/dsj-2021-028/\",\n",
        "        \"https://datascience.codata.org/articles/10.5334/dsj-2015-002/\",\n",
        "        \"https://datascience.codata.org/articles/10.5334/dsj-2015-011/\",\n",
        "        \"https://datascience.codata.org/articles/10.5334/dsj-2021-032/\",\n",
        "        \"https://datascience.codata.org/articles/10.5334/dsj-2019-055/\"]\n",
        "\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsorVUs2scAV"
      },
      "source": [
        "#Function to scrape PDF and generate abstractive of them\n",
        "def getdata(url):\n",
        "  text = []\n",
        "  r = requests.get(url)\n",
        "  d=\"\"\n",
        "  s = BeautifulSoup(r.content, 'html.parser')\n",
        "  data = ''\n",
        "  title = []\n",
        "  summary = []\n",
        "  for data in s.find_all('p'):\n",
        "    d+=data.get_text()\n",
        "    d = re.sub(r'https?:\\/\\/\\S*', ' ',d, flags = re.MULTILINE)\n",
        "    d= re.sub('[^a-zA-Z0-9]', ' ', d)\n",
        "    d=  re.sub(r\"[0-9]\", \"\", d)\n",
        "    d = d.lower()\n",
        "    d=' '.join(d.split())\n",
        "  input = tokenizer(d, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
        "  translated = model.generate(**input)\n",
        "  summary = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "  return summary"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZbvnkUeZNJE"
      },
      "source": [
        "for i in blog:\n",
        "  model_dic.append(getdata(i))"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5cY3bXVu6Jf",
        "outputId": "ccba8ff1-1571-4c65-bcb7-e922a1a0b612"
      },
      "source": [
        "model_dic"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['data management is fundamental to ensuring reproducible open scientific research however sufficient research data assistance is often not offered at universities and when offered typically only provides basic services that are viewed as optional integrating information specialists into research groups provides a potentially promising means of improving data management by providing personalized data management workflows workflows should include a file organization scheme the creation of data management roles for members a data storage sharing guide and training and evaluation librarians who regularly interact with faculty and students and are familiar with data management tools are uniquely situated to assist with the creation and assessment of these workflowsorganizing and sharing research data is a fundamental part of the research process mons that has been shown'],\n",
              " ['collection and open sharing of wastewater based epidemic data potentially provide immense public health benefits during outbreak of infectious diseases such as covid by early detection and localization of unidentified infections wastewater surveillance is expected to enable early and targeted containment of the local outbreak wastewater surveillance renders potentially high public health benefits when a small catchment is targeted however it possibly leads to stigmatization and discrimination against the targeted group therefore public commitment is crucial for the collection and open sharing of wastewater based epidemic data technical limitations and uncertainty of collected data also should be simultaneously shared on the basis of scientific communication useful application of wastewater based epidemic data is to complement clinical epidemic data which is possibly biased and'],\n",
              " ['in certain classification problems even extremely high inclusion threshold can negatively impact the classification accuracy the absence of small variance principal components can severely diminish the performance of the models we noticed this phenomenon in classification analyses using high dimension ecg data where the most common classification methods lost between and of accuracy even when using inclusion threshold however this issue can even occur in low dimension data with simple correlation structure as our numerical example shows we conclude that the exclusion of any principal components should be carefully investigatedprincipal component analysis pca du et al hsieh et al mehmet kor rek kim et al is a popular tool for data dimensionality reduction in the presence'],\n",
              " ['citizen science cs projects are part of a new era of data aggregation and harmonisation that facilitates interconnections between different datasets increasing the value and reuse of cs data has received growing attention with the appearance of the fair principles and systematic research data management rdm practises which are often promoted by university libraries however rdm initiatives in cs appear diversification and if cs have special needs in terms of rdm is unclear therefore the aim of this article is firstly to identify rdm challenges for cs projects and secondly to discuss how university libraries may support any such challengesa scoping review and a case study of danish'],\n",
              " ['sasscal was initiated to support regional weather monitoring and climate research in southern africa as a result several automatic weather stations awss were implemented to provide numerical weather data within the collaborating countries meanwhile access to the sasscal weather data is limited to a number of records that are achieved via a series of clicks currently end users can not efficaciously extract the desired weather values thus the data is not fully utilising by end users this work contributes with an open source web scraping application programming interface websapi through an interactive dashboard the objective is to extend functionalities of the sasscal weathernet for data extraction statistical data analysis and visualisation the sasscal websapi'],\n",
              " ['rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda rda'],\n",
              " ['rosa project is focused on the investigations of oil and gas industry progress in russia and other countries the primary objective is to examine and evaluate data on worldwide hydrocarbon occurrences major aim is to construct a comprehensive map of the allocation of oil and gas fields with large reserves for further analogy estimation and reconstruction of geological historythe main contribution of this work is the development of a multidimensional and multilevel database and the corresponding gis project for visualization the set of multidisciplinary backgrounds in combination with a spatial algorithmic tools are used as a basis for an analytical study of worldwide hydrocarbon occurrences and estimation establishment of petroleum industrycreating and support of verified databases is one'],\n",
              " ['the international nucleotide sequence database collaboration insdc permanently guarantees free and unrestricted access to nucleotide sequence data for all researchersIrrespective of nationality or affiliation however recent virus information is primarily distributed via the restricted access repository known as the global initiative on sharing avian flu data gisaid supported by the world health organization as compensation for the restriction gisaid needs to meet its initial goal of benefit sharing among countries and to curb ongoing vaccine diplomacy campaignsthe collaboration among nucleotide sequence databases began in and involved the data library at the european molecular biology laboratory embl heidelberg germany and genbank at the los alamos science laboratory now called the los alamos national'],\n",
              " ['high quality data are the prerequisite for analyzing and using big data and for guaranteeing the value of the data currently comprehensive analysis and research of quality standards and quality assessment methods for big data are lacking first this paper summarizes reviews of data quality research second this paper analyzes the data characteristics of the big data environment presents quality challenges faced by big data and formulates a hierarchical data quality framework from the perspective of data users this framework consists of big data quality dimensions quality characteristics and quality indexes finally on the basis of this framework this paper constructs a dynamic assessment process for data quality this process has good expansibility and adaptability and can meet the needs of big data'],\n",
              " ['astrostatistics and astroinformatics are vital for dealing with the big data issues now faced by astronomy like other disciplines in the big data era astronomy has many v characteristics in this paper we list the different data mining algorithms used in astronomy along with data mining software and tools related to astronomical applications we present sdss a project often referred to by other astronomical projects as the most successful sky survey in the history of astronomy and describe the factors influencing its success we also discuss the success of astrostatistics and astroinformatics organizations and the conferences and summer schools on these issues that are held annually all the above indicates that astronomy and scientists from other areas are'],\n",
              " ['machine actionable dmps are a way to exchange and act on information about data used and produced by researchers we need all stakeholders to collaborate and synchronise their efforts the basic framework requires i an application profile for representing information in a common way ii services that can provide and use this information in an automated way the application profile is the focus of this paper and can be defined as a metadata design specification that uses a selection of terms from multiple metadata vocabularies with added constraints to meet application specific requirementsthe research data'],\n",
              " [': Dataset consists of videos related to different subjects the videos contain similar text the text contains digits from to recited by different subjects using the same experimental setup this dataset can be used as a unique resource for researchers and analysts for training deep neural networks to build highly efficient and accurate recognition models in various domains of computer vision such as face recognition model expression recognition model speech recognition model text recognition etc in this paper we also train models related to face recognition and speech recognition on our dataset and also compare the results with the publically available datasets to show the effectiveness of our dataset the experimental results show that our comprehensive dataset is more accurate than other dataset on which']]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIw3OywaY7UR"
      },
      "source": [
        "The above output is the predicted summary generated by our function getdataurl of 12 articles applied as a input. The text extracted from the online articles were first clean and then pass as a input to the model for abstractive summary generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpvU5XAgrOm6"
      },
      "source": [
        "#Function that scrape only abstracts so that it can be used as original summary\n",
        "def original_summary(url):\n",
        "  r = requests.get(url)\n",
        "  sum = []\n",
        "  text = []\n",
        "  d=\"\"\n",
        "  soup = BeautifulSoup(r.content, 'html.parser')\n",
        "  for data in soup.find_all(\"p\")[1]:\n",
        "    d = re.sub(r'https?:\\/\\/\\S*', \"\",str(data))\n",
        "    d= re.sub(r'[^a-zA-Z0-9]', \" \", d)\n",
        "    d=  re.sub(r\"[0-9]\", \" \",d)\n",
        "    d = d.lower()\n",
        "    d=' '.join(d.split())\n",
        "    sum.append(d)\n",
        "  return(sum)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qldSinsF9mL3"
      },
      "source": [
        "originalsummary = []\n",
        "summary = []\n",
        "for i in blog:\n",
        "  summary+=original_summary(i)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30GwrqGW-Xbr",
        "outputId": "e83a9cca-43ad-440e-9b83-33827c005ee6"
      },
      "source": [
        "print(summary)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehensive research data management is fundamental to ensuring reproducible open scientific research however sufficient research data assistance is often not offered at universities and when offered typically only provides basic services that are viewed as optional integrating information specialists into research groups provides a potentially promising means of improving data management by providing personalized data management workflows workflows are comprehensive executable guides that require planning implementation feedback and adaptation comprehensive data management workflows should include a file organization scheme the creation of data management roles for members a data storage sharing guide and training and evaluation librarians who regularly interact with faculty and students and are familiar with data management tools are uniquely situated to assist with the creation and assessment of these workflows', 'collection and open sharing of wastewater based epidemic data potentially provide immense public health benefits during outbreak of infectious diseases such as covid by early detection and localization of unidentified infections wastewater surveillance is expected to enable early and targeted containment of the local outbreak wastewater surveillance renders potentially high public health benefits when a small catchment is targeted however it possibly leads to stigmatization and discrimination against the targeted group therefore public commitment is crucial for the collection and open sharing of wastewater based epidemic data with respect to the sharing of wastewater based epidemic data technical limitations and uncertainty of collected data also should be simultaneously shared on the basis of scientific communication useful application of wastewater based epidemic data is to complement clinical epidemic data which is possibly biased and overlooks unidentified infections to acquire public commitment toward the collection and open sharing of wastewater based epidemic data stakeholders need to reach a consensus on possible options of restrictive measures taken with respect to the collected data as well as appropriate handling of the collected data to prevent stigmatization and discrimination', 'principal component analysis pca is a commonly used technique that uses the correlation structure of the original variables to reduce the dimensionality of the data this reduction is achieved by considering only the first few principal components for a subsequent analysis the usual inclusion criterion is defined by the proportion of the total variance of the principal components exceeding a predetermined threshold we show that in certain classification problems even extremely high inclusion threshold can negatively impact the classification accuracy the omission of small variance principal components can severely diminish the performance of the models we noticed this phenomenon in classification analyses using high dimension ecg data where the most common classification methods lost between and of accuracy even when using inclusion threshold however this issue can even occur in low dimension data with simple correlation structure as our numerical example shows we conclude that the exclusion of any principal components should be carefully investigated', 'citizen science cs projects are part of a new era of data aggregation and harmonisation that facilitates interconnections between different datasets increasing the value and reuse of cs data has received growing attention with the appearance of the fair principles and systematic research data management rdm practises which are often promoted by university libraries however rdm initiatives in cs appear diversified and if cs have special needs in terms of rdm is unclear therefore the aim of this article is firstly to identify rdm challenges for cs projects and secondly to discuss how university libraries may support any such challenges', 'the southern african science service centre for climate and land management sasscal was initiated to support regional weather monitoring and climate research in southern africa as a result several automatic weather stations awss were implemented to provide numerical weather data within the collaborating countries meanwhile access to the sasscal weather data is limited to a number of records that are achieved via a series of clicks currently end users can not efficaciously extract the desired weather values thus the data is not fully utilised by end users this work contributes with an open source web scraping application programming interface websapi through an interactive dashboard the objective is to extend functionalities of the sasscal weathernet for data extraction statistical data analysis and visualisation the sasscal websapi was developed using the r statistical environment it deploys web scraping and data wrangling techniques to support access to sasscal weather data this websapi reduces the risk of human error and the researcher s effort of generating desired data sets the proposed framework for the sasscal websapi can be modified for other weather data banks while taking into consideration the legality and ethics of the toolkit', 'some of the early research data alliance working groups reused the notion of digital objects as digital entities described by metadata and referenced by a persistent identifier in recent times the fair principles became a prominent role as framework for the sustainability of scientific data both approaches had always machine actionability the capability of computational systems to use services on data without human intervention in their focus the more technical approach of digital objects turned out to provide a complementary view on several aspects of the policy framework of fair from a technical perspective after a deeper analysis and integration of these concepts by a group of european data experts the discussion intensified on so called fair digital objects but they need to be accompanied by services as building blocks for automated processes we will describe the components of this framework and its potentials here and also which services inside this framework are required', 'the rosa project is focused on the investigations of oil and gas industry progress in russia and other countries the primary objective is to examine and evaluate data on worldwide hydrocarbon occurrences major aim is to construct a comprehensive map of the allocation of oil and gas fields with large reserves for further analogy estimation and reconstruction of geological history', 'open access free access and the public domain are different concepts the international nucleotide sequence database collaboration insdc permanently guarantees free and unrestricted access to nucleotide sequence data for all researchers irrespective of nationality or affiliation however recent virus information is primarily distributed via the restricted access repository known as the global initiative on sharing avian flu data gisaid supported by the world health organization as compensation for the restriction gisaid needs to meet its initial goal of benefit sharing among countries and to curb ongoing vaccine diplomacy campaigns', 'high quality data are the precondition for analyzing and using big data and for guaranteeing the value of the data currently comprehensive analysis and research of quality standards and quality assessment methods for big data are lacking first this paper summarizes reviews of data quality research second this paper analyzes the data characteristics of the big data environment presents quality challenges faced by big data and formulates a hierarchical data quality framework from the perspective of data users this framework consists of big data quality dimensions quality characteristics and quality indexes finally on the basis of this framework this paper constructs a dynamic assessment process for data quality this process has good expansibility and adaptability and can meet the needs of big data quality assessment the research results enrich the theoretical scope of big data and lay a solid foundation for the future by establishing an assessment model and studying evaluation algorithms', 'the fields of astrostatistics and astroinformatics are vital for dealing with the big data issues now faced by astronomy like other disciplines in the big data era astronomy has many v characteristics in this paper we list the different data mining algorithms used in astronomy along with data mining software and tools related to astronomical applications we present sdss a project often referred to by other astronomical projects as the most successful sky survey in the history of astronomy and describe the factors influencing its success we also discuss the success of astrostatistics and astroinformatics organizations and the conferences and summer schools on these issues that are held annually all the above indicates that astronomers and scientists from other areas are ready to face the challenges and opportunities provided by massive data volume', 'this paper presents the application profile for machine actionable data management plans that allows information from traditional data management plans to be expressed in a machine actionable way we describe the methodology and research conducted to define the application profile we also discuss design decisions made during its development and present systems which have adopted it the application profile was developed in an open and consensus driven manner within the dmp common standards working group of the research data alliance and is its official recommendation', 'this paper presents a comprehensive highly defined and fully labelled video dataset this dataset consists of videos related to different subjects the videos contain similar text and the text contains digits from to recited by different subjects using the same experimental setup this dataset can be used as a unique resource for researchers and analysts for training deep neural networks to build highly efficient and accurate recognition models in various domains of computer vision such as face recognition model expression recognition model speech recognition model text recognition etc in this paper we also train models related to face recognition and speech recognition on our dataset and also compare the results with the publically available datasets to show the effectiveness of our dataset the experimental results show that our comprehensive dataset is more accurate than other dataset on which the models are tested']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZgSi2z0aC_8"
      },
      "source": [
        "The above output is the original summary of the online articles that were extracted using original_function and stored in summary list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixkaYakJmfTE"
      },
      "source": [
        "#Function that calculate score all the summaries \n",
        "def rougescorer(b):\n",
        "  precision= []\n",
        "  recall=[]\n",
        "  fmeasure=[]\n",
        "  a=[]\n",
        "  b=[]\n",
        "  c=[]\n",
        "  colnames = ['precision','recall','fmeasure']\n",
        "  df = pd.DataFrame(columns =colnames)\n",
        "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "  for b in range(0,12):\n",
        "    scores = scorer.score(str(summary[b]),str(model_dic[b]))\n",
        "    a=(scores['rouge1'].precision)\n",
        "    b=(scores['rouge1'].recall)\n",
        "    c=(scores['rouge1'].fmeasure)\n",
        "    precision.append(a)\n",
        "    recall.append(b)\n",
        "    fmeasure.append(c)\n",
        "  df['precision'] = precision\n",
        "  df['recall'] = recall\n",
        "  df['fmeasure'] = fmeasure\n",
        "  return df"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dW2uZowDgM0"
      },
      "source": [
        "import pandas as pd\n",
        "for j in blog:\n",
        "  eval_score=rougescorer(j)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTP3ze_qNYgl",
        "outputId": "e9464bc4-d66c-4fdd-8ae2-b94f5abcfb34"
      },
      "source": [
        "print(eval_score)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    precision    recall  fmeasure\n",
            "0    0.887097  0.887097  0.887097\n",
            "1    1.000000  0.668478  0.801303\n",
            "2    0.844828  0.632258  0.723247\n",
            "3    0.907407  0.980000  0.942308\n",
            "4    1.000000  0.596859  0.747541\n",
            "5    0.000000  0.000000  0.000000\n",
            "6    0.500000  0.983333  0.662921\n",
            "7    0.687500  0.865169  0.766169\n",
            "8    0.991870  0.802632  0.887273\n",
            "9    0.991525  0.879699  0.932271\n",
            "10   0.410526  0.458824  0.433333\n",
            "11   1.000000  0.873239  0.932331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfOJfG4W_xYJ"
      },
      "source": [
        "column_names = [\"model\", \"data_source\", \"humansummary\", \"predicted_summary\"]\n",
        "reddit_model_eval = pd.DataFrame(columns = column_names)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMRi5l13_59C"
      },
      "source": [
        "mod = 'Reddit_tifu'\n",
        "data_source = 'articles'"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFwRirv__00T"
      },
      "source": [
        "reddit_model_eval['humansummary'] =summary\n",
        "reddit_model_eval['model'] = mod\n",
        "reddit_model_eval['data_source'] = data_source\n",
        "reddit_model_eval['predicted_summary'] =model_dic"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UKEm5-ABYjc"
      },
      "source": [
        "roughscore = 'rouge1'\n",
        "reddit_model_eval['roughscorer'] = roughscore\n",
        "reddit_model_eval['precision'] = eval_score.precision\n",
        "reddit_model_eval['recall'] = eval_score.recall\n",
        "reddit_model_eval['fmeasure'] = eval_score.fmeasure"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2HOCjMonmPF"
      },
      "source": [
        "human_evaluation = [4,3,3,5,3,0,2,3,4,5,2,5]\n",
        "reddit_model_eval['human_evaluation'] = human_evaluation"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "iwgNqmvJB4bv",
        "outputId": "c9cbba58-2cea-4c50-a0c7-2aa4dd22905d"
      },
      "source": [
        "reddit_model_eval"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>data_source</th>\n",
              "      <th>humansummary</th>\n",
              "      <th>predicted_summary</th>\n",
              "      <th>roughscorer</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>fmeasure</th>\n",
              "      <th>human_evaluation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Reddit_tifu</td>\n",
              "      <td>articles</td>\n",
              "      <td>comprehensive research data management is fund...</td>\n",
              "      <td>[data management is fundamental to ensuring re...</td>\n",
              "      <td>rouge1</td>\n",
              "      <td>0.887097</td>\n",
              "      <td>0.887097</td>\n",
              "      <td>0.887097</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Reddit_tifu</td>\n",
              "      <td>articles</td>\n",
              "      <td>collection and open sharing of wastewater base...</td>\n",
              "      <td>[collection and open sharing of wastewater bas...</td>\n",
              "      <td>rouge1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.668478</td>\n",
              "      <td>0.801303</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Reddit_tifu</td>\n",
              "      <td>articles</td>\n",
              "      <td>principal component analysis pca is a commonly...</td>\n",
              "      <td>[in certain classification problems even extre...</td>\n",
              "      <td>rouge1</td>\n",
              "      <td>0.844828</td>\n",
              "      <td>0.632258</td>\n",
              "      <td>0.723247</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Reddit_tifu</td>\n",
              "      <td>articles</td>\n",
              "      <td>citizen science cs projects are part of a new ...</td>\n",
              "      <td>[citizen science cs projects are part of a new...</td>\n",
              "      <td>rouge1</td>\n",
              "      <td>0.907407</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reddit_tifu</td>\n",
              "      <td>articles</td>\n",
              "      <td>the southern african science service centre fo...</td>\n",
              "      <td>[sasscal was initiated to support regional wea...</td>\n",
              "      <td>rouge1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.596859</td>\n",
              "      <td>0.747541</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Reddit_tifu</td>\n",
              "      <td>articles</td>\n",
              "      <td>some of the early research data alliance worki...</td>\n",
              "      <td>[rda rda rda rda rda rda rda rda rda rda rda r...</td>\n",
              "      <td>rouge1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Reddit_tifu</td>\n",
              "      <td>articles</td>\n",
              "      <td>the rosa project is focused on the investigati...</td>\n",
              "      <td>[rosa project is focused on the investigations...</td>\n",
              "      <td>rouge1</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.983333</td>\n",
              "      <td>0.662921</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Reddit_tifu</td>\n",
              "      <td>articles</td>\n",
              "      <td>open access free access and the public domain ...</td>\n",
              "      <td>[the international nucleotide sequence databas...</td>\n",
              "      <td>rouge1</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.865169</td>\n",
              "      <td>0.766169</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Reddit_tifu</td>\n",
              "      <td>articles</td>\n",
              "      <td>high quality data are the precondition for ana...</td>\n",
              "      <td>[high quality data are the prerequisite for an...</td>\n",
              "      <td>rouge1</td>\n",
              "      <td>0.991870</td>\n",
              "      <td>0.802632</td>\n",
              "      <td>0.887273</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Reddit_tifu</td>\n",
              "      <td>articles</td>\n",
              "      <td>the fields of astrostatistics and astroinforma...</td>\n",
              "      <td>[astrostatistics and astroinformatics are vita...</td>\n",
              "      <td>rouge1</td>\n",
              "      <td>0.991525</td>\n",
              "      <td>0.879699</td>\n",
              "      <td>0.932271</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Reddit_tifu</td>\n",
              "      <td>articles</td>\n",
              "      <td>this paper presents the application profile fo...</td>\n",
              "      <td>[machine actionable dmps are a way to exchange...</td>\n",
              "      <td>rouge1</td>\n",
              "      <td>0.410526</td>\n",
              "      <td>0.458824</td>\n",
              "      <td>0.433333</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Reddit_tifu</td>\n",
              "      <td>articles</td>\n",
              "      <td>this paper presents a comprehensive highly def...</td>\n",
              "      <td>[: Dataset consists of videos related to diffe...</td>\n",
              "      <td>rouge1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.873239</td>\n",
              "      <td>0.932331</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          model data_source  ...  fmeasure human_evaluation\n",
              "0   Reddit_tifu    articles  ...  0.887097                4\n",
              "1   Reddit_tifu    articles  ...  0.801303                3\n",
              "2   Reddit_tifu    articles  ...  0.723247                3\n",
              "3   Reddit_tifu    articles  ...  0.942308                5\n",
              "4   Reddit_tifu    articles  ...  0.747541                3\n",
              "5   Reddit_tifu    articles  ...  0.000000                0\n",
              "6   Reddit_tifu    articles  ...  0.662921                2\n",
              "7   Reddit_tifu    articles  ...  0.766169                3\n",
              "8   Reddit_tifu    articles  ...  0.887273                4\n",
              "9   Reddit_tifu    articles  ...  0.932271                5\n",
              "10  Reddit_tifu    articles  ...  0.433333                2\n",
              "11  Reddit_tifu    articles  ...  0.932331                5\n",
              "\n",
              "[12 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br9ekYelXUqZ"
      },
      "source": [
        "In the above output, I compare the predicted summary with the human summary, here human summary is the abstract of articles. So, I calculate the score by applying the scorer function on the column human summary and predicted_summary and obtained high value of F1 score. For e.g. for the article 11, value of F1 score is 0.932 which clearly states that our predicted summary and human summary are very much similar to each other, similarly for the article 10, F1 score is 0.43 which shows that human summary and predicted are not related to each other. There is also a case where our model  was not able to predict summary, article 5 shows that model was unable to predict summary for that, so its precision, recall and F1 score are all zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI4Kr9wDTZh9",
        "outputId": "703acf70-56e7-46a1-c1ee-b7dd8bb7e3ce"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjPV2Da3TzjV"
      },
      "source": [
        "with open('/gdrive/My Drive/reddit_tifu.csv', 'w') as f:\n",
        "  reddit_model_eval.to_csv(f)"
      ],
      "execution_count": 81,
      "outputs": []
    }
  ]
}